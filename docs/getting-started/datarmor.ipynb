{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad6d8b4-ad8b-4080-980f-70937a46c414",
   "metadata": {},
   "source": [
    "# Starting Dask cluster and client using dask-hpcconfig on Datarmor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce60956-3cf4-45c7-88e0-8f13cf3b5395",
   "metadata": {},
   "source": [
    "dask-hpcconfig wraps certain configuration for optimam usage of dask on HPC clusters for users, so that each users does not need to specify manually.  \n",
    "\n",
    "This notebook shows ways to start different size of dask clusters on datarmor using dask-hpcconfg.  \n",
    "\n",
    "- Example 1: Single node  (in default mode) \n",
    "- Example 2: single node  (with specification of size of memory for each worker) \n",
    "- Example 3: Starting dask workers on different compute nodes.  (in default mode) \n",
    "- Example 4: Starting dask workers on different compute nodes.  (with specification of size of memory for each worker) \n",
    "- Example 5: Some advanced use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee916a-c475-4abe-8a57-bfd2779275b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_hpcconfig\n",
    "from distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91fb52-6fcf-44c0-a179-7c782ea54a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_hpcconfig.print_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40277811-8683-4005-88af-f79261b661e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_hpcconfig.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0d2b0-d41d-4215-b6c3-de2c9ff62c88",
   "metadata": {},
   "source": [
    "## Example 1: Single node\n",
    "\n",
    "This use case will run 14 workers on your local datarmor nodes.  It will set several optimnisation for you ( spill=false, optimal number of threads for each workers...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d99d50-3567-43ba-9ef3-4488401914ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster = dask_hpcconfig.cluster(\"datarmor-local\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d2956-9da9-4d9c-bc61-ef5d9915f1c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Click here the 'Launch dashboard in JupyterLab to see how your dask workers are allocated. \n",
    "You are ready to use dask in your enviroment now.  \n",
    "Before trying next example, do not forget to close your cluster.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430ad4b-b619-43ef-9d0a-aa1cda8ede43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b0c8c-c684-462a-bb3f-e5b5c80b4686",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Example 2: Big-mem single node\n",
    "\n",
    "This example show the use case that requires max 25GB of memory for each worker"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7420c3dc-e492-4948-bf71-ea5b085fbd8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### this line should work later instead of the next line, with guessing the n_workers\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor-local\", **{\"cluster.worker_memory\": \"18GB\"})\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd6072-cd9b-4cbe-b66d-e8fefc769adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory_size = 25\n",
    "n_worker_per_node = 115 // memory_size\n",
    "n_threads_per_worker = 28 // n_worker_per_node\n",
    "print(n_worker_per_node, n_threads_per_worker)\n",
    "overrides = {\n",
    "    \"cluster.threads_per_worker\": n_threads_per_worker,\n",
    "    \"cluster.n_workers\": n_worker_per_node,\n",
    "}\n",
    "\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor-local\", **overrides)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a358c-73b0-4083-ab72-548529d18f31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd966976-5610-4ca6-86c1-47cbd31b2a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Example 3: Multiple node to run dask workers\n",
    "This use case will run 14 workers on each node of Datarmor.  It will set several optimnisation for you ( spill=false, optimal number of threads for each workers...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bb236-b765-4a7d-a8ed-b7ade53696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrides = {}\n",
    "# overrides = { \"cluster.cores\": 28 , \"cluster.processes\": 6 }\n",
    "\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor\", **overrides)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b32922-4a29-489e-9074-a06591695b09",
   "metadata": {},
   "source": [
    "### Here you specify, how many workers you want in total\n",
    "This example asks 20 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9058b5-9ca4-4f51-81cb-bd692a4c1b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.scale(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c20c7e-bb12-40d5-a800-470115bf91aa",
   "metadata": {},
   "source": [
    "#### You can use qstat to verify the jobs submitted in datarmor\n",
    "You will see there are two job submitted, and there are in fact 28 workers stated.  It is because we asked for '14 workers' for each node.  Thus to make it 20, it must at least start 2 jobs to have 20, thus you got 28 in total.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f806ef0-ea89-469c-a9b6-33913a5fd7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!qstat -u todaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f98fc-e9f3-4324-ad45-ed939693d518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03ccae-1d9d-42ae-910b-a99f2e5d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6f999-f2f7-43f5-bb0f-47dcd52b8b19",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Example 4: Big-mem single node\n",
    "\n",
    "This example show the use case that requires max 25GB of memory for each worker"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b39a6fc8-da47-4c54-9ebf-2d8718a65969",
   "metadata": {
    "tags": []
   },
   "source": [
    "### this line should work later instead of the next line, with guessing the n_workers\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor\", **{\"cluster.worker_memory\": \"25GB\"})\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee902fe-3551-4592-a7e8-c8bb04d7f98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory_size = 25\n",
    "n_worker_per_node = 120 // memory_size\n",
    "# n_threads_per_worker=28//n_worker_per_node\n",
    "print(\"nb workers we use for each node \", n_worker_per_node)  # ,n_threads_per_worker)\n",
    "overrides = {\n",
    "    \"cluster.processes\": n_worker_per_node\n",
    "}  # ,\"cluster.c\": n_worker_per_node }\n",
    "\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor\", **overrides)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917f0d2-1342-415d-bc9e-d71a8f9e21bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97b059-b5e8-4e77-a112-aec264c165ad",
   "metadata": {},
   "source": [
    "#### You can use qstat to verify the jobs submitted in datarmor\n",
    "You will see there are two job submitted, and there are in fact 28 workers stated.  It is because we asked for '14 workers' for each node.  Thus to make it 20, it must at least start 2 jobs to have 20, thus you got 28 in total.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ae746-3492-45b9-b43a-99459695eba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!qstat -u todaka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db7243-7777-4621-b99c-5426fb7cd0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c900a-4b08-4e6c-b357-a03950ac6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852368d-8bc2-4f80-9f40-2a0366089ef2",
   "metadata": {},
   "source": [
    "## Example 5: advanced usage.  \n",
    "For advanced users, you can pass different dask cluster, or worker options \n",
    "- 'datarmor-local': https://docs.dask.org/en/stable/deploying-python.html#reference\n",
    "- 'datarmor': \n",
    "-  for all, worker specification: https://distributed.dask.org/en/stable/worker.html#api-documentation\n",
    "\n",
    "Next cells shows for the case that you want change 'distributed.worker.memory.pause' option to 80%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c951d-8590-4141-b8c2-4fbb3428a944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one can specify mo\n",
    "overrides = {\"cluster.cores\": 3, \"distributed.worker.memory.pause\": 0.80}\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor-local\", **overrides)\n",
    "cluster = dask_hpcconfig.cluster(\"datarmor-local\", **overrides)\n",
    "client = Client(cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
